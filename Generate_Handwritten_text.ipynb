{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aallali/OCR_projects/GDC'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from Preprocessing import Preprocessing\n",
    "from Handwriting_gen import Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create instances of relevant classes\n",
    "From Preprocessing and Handwriting_gen, we use \"Preprocessing\" class to preprocess and adjust the images we're handling, they can be images of handwritten characters or handwritten text (string).<p>\n",
    "From Handwriting_gen, we use \"Generation\" class to fetch, handle and process images of single characters in order to produce a set of images containing handwritten text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing= Preprocessing()\n",
    "generation= Generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we set the necessary attributes of each class (For more info, check out the init() function of the class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir= \"Data/HTR/Chars74K_dataset/English/Hnd/Img/\"\n",
    "\n",
    "generation.source_dir= source_dir\n",
    "generation.special_char_dir= \"Data/HTR/special_characters/\"\n",
    "\n",
    "generation.dilation_kernel = np.ones((2,2), np.uint8)\n",
    "\n",
    "generation.charW= 15\n",
    "generation.charH= 25\n",
    "generation.char_capital_W= 25\n",
    "generation.char_capital_H= 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate handwritten text images for text recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a text recognition task, we often need two inputs:<p>\n",
    "- A set of images containing text to recognize.\n",
    "- list of strings corresponding to text inside these images (ground truth text).<p>\n",
    " In our case, we'll generate images containing handwritten text and text (.txt) file that contains the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_seq_to_string(ascii_seq):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - ascii_seq: ascii numbers array corresponding to an array of \n",
    "                characters (e.g. [97, 90, 53] <-> ['a', 'Z', '5'])\n",
    "    Outputs:\n",
    "    - string: a string that assembles the characters corresponding to \n",
    "    the ASCII sequence (e.g. ascii_seq= [97, 90, 53] -> string= 'aZ5')\n",
    "    \"\"\"\n",
    "    \n",
    "    string= \"\"\n",
    "    for n in ascii_seq:\n",
    "        string= string+ str(chr(n))\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A path named \"saving_dir\", in which the generated images can be saved, should be specified. A text file will be created in the parent folder of this file.<p>\n",
    "Also, the size of generated images dataset should be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_dir= \"Data/HTR/Generated_sequences/text_recognition/train/\"\n",
    "gt_txt_file = open(os.path.join(str(Path(saving_dir).parent), 'train_gt.txt'), \"w+\")\n",
    "dataset_size= 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handwritten text dataset generation is ready to be launched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "\n",
    "for i in range(dataset_size):\n",
    "    #1 element capital letters sequence (A->Z)\n",
    "    seq1= random.choices(range(65, 91), k=1)\n",
    "\n",
    "    #2 element capital letters sequence (A->Z)\n",
    "    seq2= random.choices(range(65, 91), k=2)\n",
    "\n",
    "    #1 element letters sequence (a->z)\n",
    "    seq3= random.choices(range(97, 122), k=1)\n",
    "\n",
    "    #2 element letters sequence (a->z)\n",
    "    seq4= random.choices(range(97, 122), k=2)\n",
    "    for seq in [seq1, seq2, seq3, seq4]:\n",
    "\n",
    "        string = ascii_seq_to_string(seq)\n",
    "        try:\n",
    "            string_img= generation.generate_sentence_image(string)\n",
    "            cv2.imwrite(os.path.join(saving_dir, f\"Generated_img_{i+j}.jpg\"), string_img)\n",
    "            gt_txt_file.write(f\"Generated_img_{i+j}.jpg\\t{string}\\n\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        j+=1\n",
    "\n",
    "gt_txt_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate handwritten text images for text segmentation\n",
    "For an image segmentation task, we often need two inputs:<p>\n",
    "- A set of images containing text to recognize.\n",
    "- list of annotations corresponding to the regions of interest in each image, the annotations should follow a certain format (COCO, VOC XML, ...), to be determined by the user.<p>\n",
    " In our case, we'll generate handwritten text images and a json file containing the annotations in COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation.dilation_kernel = np.ones((2,2),np.uint8)\n",
    "generation.keyword= \".png\"\n",
    "\n",
    "generation.charW= 25\n",
    "generation.charH= 35\n",
    "generation.char_capital_W= 25\n",
    "generation.char_capital_H= 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_dir= \"Data/HTR/Generated_sequences/test_annotated_images/\"\n",
    "json_file= 'test_coco_annotations.json'\n",
    "\n",
    "len_dataset= 2000\n",
    "\n",
    "img_id=0\n",
    "letter_id=0\n",
    "coco_dict={}\n",
    "coco_dict[\"licenses\"]= []\n",
    "coco_dict[\"info\"]= {}\n",
    "coco_dict[\"images\"]= []\n",
    "coco_dict[\"annotations\"]= []\n",
    "\n",
    "coco_dict[\"categories\"]= [{\"supercategory\": \"\",\n",
    "                            \"id\": 1,\n",
    "                            \"keypoints\": [],\n",
    "                            \"name\": \"text\"}\n",
    "                          ]\n",
    "\n",
    "for i in range(len_dataset//7):\n",
    "    # Cable Name in ASCII (3letters+ 4numbers + 3letters)\n",
    "    seq1= random.choices(range(65, 91), k=3) + random.choices(range(48, 58), k=4) + random.choices(range(65, 91), k=3)\n",
    "\n",
    "    #Indices in ASCII (from A->Z)\n",
    "    seq2= random.choices(range(65, 91), k=1)\n",
    "\n",
    "    # Plan applicable in ASCII (3letters+ \"315\"+ 3numbers)\n",
    "    seq3= random.choices(range(65, 91), k=3) + [51,49, 53] + random.choices(range(48, 58), k=3)\n",
    "\n",
    "    # Dates (jj/mm/aaaa) 2010 <aaaa< 2020\n",
    "    seq4= random.choices(range(48, 58), k=2)+ [\"/\"]+ random.choices(range(48, 58), k=2)+ [\"/\"]+ [50, 48]+ random.choices(range(49, 51), k=1)+ random.choices(range(48, 58), k=1)\n",
    "\n",
    "    # Dates (jj/mm/aa) 10 <aa< 20\n",
    "    seq5= random.choices(range(48, 58), k=2)+ [\"/\"]+ random.choices(range(48, 58), k=2)+ [\"/\"]+ random.choices(range(49, 51), k=1)+ random.choices(range(48, 58), k=1)\n",
    "\n",
    "    # Plan applicable in ASCII (3letters+ \"313\"+ 3 numbers)\n",
    "    seq6= random.choices(range(65, 91), k=3) + [51,49, 51] + random.choices(range(48, 58), k=3)\n",
    "    \n",
    "    # Cable Name in ASCII (3letters+ 4numbers + 3letters+ /+ 1letter)\n",
    "    seq7= random.choices(range(65, 91), k=3) + random.choices(range(48, 58), k=4) + random.choices(range(65, 91), k=3)+ [\"/\"]+ random.choices(range(49, 51), k=1)\n",
    "\n",
    "#     seq8= random.choices(range(65, 91), k=3) + random.choices(range(48, 58), k=4) + random.choices(range(65, 91), k=3)+ [\"/\"]+ random.choices(range(49, 51), k=1)\n",
    "    \n",
    "    for seq in [seq1, seq2, seq3, seq4, seq5, seq6, seq7]:\n",
    "        string = ascii_seq_to_string(seq)\n",
    "        img_id+=1\n",
    "\n",
    "        try:\n",
    "            margin= 30\n",
    "            string= string.replace(\" \", \"\")\n",
    "            string_img= generation.generate_sentence_image(string)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        string_img= preprocessing.add_border(img=string_img, bordersize= margin)\n",
    "\n",
    "\n",
    "        #Save the generated image \n",
    "        cv2.imwrite(os.path.join(saving_dir, f\"Generated_img_{img_id}.jpg\"), string_img)\n",
    "        height, width= string_img.shape[:2]\n",
    "        \n",
    "        #Generate the COCO annotations: image filename, regions coordinates, ...\n",
    "        img_dict= {\"file_name\": f\"Generated_img_{img_id}.jpg\", \"id\": img_id,\n",
    "                   \"width\": width, \"height\": height, \n",
    "                   \"date_captured\": \"\", \"flickr_url\": \"\", \"license\": 0, \"coco_url\": \"\"}\n",
    "        coco_dict[\"images\"].append(img_dict)\n",
    "\n",
    "        for letter in range(len(string)):\n",
    "            letter_id+=1\n",
    "\n",
    "            h= generation.charH\n",
    "            w= generation.charW\n",
    "            (x0, y0)= ( letter*w+margin, margin )\n",
    "            (x1, y1)= ( (letter+1)*w + margin, margin )\n",
    "            (x2, y2)= ( (letter+1)*w + margin, h+ margin )\n",
    "            (x3, y3)= ( letter*w+margin , h+ margin )\n",
    "            coco_dict[\"annotations\"].append({'image_id':img_id, \n",
    "                                             \"segmentation\": [[x0, y0, x1, y1,\n",
    "                                                               x2, y2, x3, y3\n",
    "                                                              ]],\n",
    "                                             \"bbox\": [x0, y0, w, h],\n",
    "#                                              \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                                              \"category_id\": 1,\n",
    "                                             \"id\": letter_id,\n",
    "                                             \"area\": w*h,\n",
    "                                             \"iscrowd\": 0\n",
    "                                            })\n",
    "\n",
    "# Save the COCO annotations in the json file  \n",
    "with open(os.path.join(str(Path(saving_dir).parent), json_file), \"w\") as f:\n",
    "            json.dump(coco_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_text",
   "language": "python",
   "name": "deep_text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
